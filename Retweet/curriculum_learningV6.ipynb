{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54180a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison de 4 stratégies d'entraînement\n",
      "==================================================\n",
      "Chargement des données...\n",
      "Données chargées: 4000 train, 2000 validation\n",
      "\n",
      "Création des buckets pour les 4 stratégies...\n",
      "\n",
      "================================================================================\n",
      "ENTRAÎNEMENT: Curriculum LearningV2\n",
      "================================================================================\n",
      "Bucket 1: 1000 séquences, longueurs [50-264], moyenne: 101.0\n",
      "Bucket 2: 2000 séquences, longueurs [50-264], moyenne: 113.5\n",
      "Bucket 3: 3000 séquences, longueurs [50-264], moyenne: 115.1\n",
      "Bucket 4: 4000 séquences, longueurs [50-264], moyenne: 108.9\n",
      "\n",
      "Bucket 1/4:\n",
      "  Train: 1000 séquences\n",
      "  Validation: 250 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 851.159020, Val loss/event: 1458.094311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 199.119487, Val loss/event: 428.490496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 76.295929, Val loss/event: 204.214286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 41.942284, Val loss/event: 120.278547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 27.667263, Val loss/event: 79.517932\n",
      "\n",
      "Bucket 2/4:\n",
      "  Train: 2000 séquences\n",
      "  Validation: 500 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 26.915579, Val loss/event: 27.208086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 14.011045, Val loss/event: 16.554058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 9.987981, Val loss/event: 12.062703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 8.359772, Val loss/event: 10.033114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 7.592859, Val loss/event: 8.968756\n",
      "\n",
      "Bucket 3/4:\n",
      "  Train: 3000 séquences\n",
      "  Validation: 750 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 7.603655, Val loss/event: 7.746929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 7.052805, Val loss/event: 7.262304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 6.811859, Val loss/event: 7.106702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 6.638520, Val loss/event: 6.868956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 6.558652, Val loss/event: 6.779350\n",
      "\n",
      "Bucket 4/4:\n",
      "  Train: 4000 séquences\n",
      "  Validation: 1000 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 6.711716, Val loss/event: 6.656293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 6.626264, Val loss/event: 6.560621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 6.556990, Val loss/event: 6.518252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 6.532671, Val loss/event: 6.527043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 6.507782, Val loss/event: 6.509722\n",
      "Modèle sauvegardé: Comparison_ModelsV6/Curriculum_LearningV2_finalV6.pt\n",
      "\n",
      "================================================================================\n",
      "ENTRAÎNEMENT: Random Buckets\n",
      "================================================================================\n",
      "Bucket 1: 1000 séquences, longueurs [50-262], moyenne: 106.2\n",
      "Bucket 2: 2000 séquences, longueurs [50-264], moyenne: 107.8\n",
      "Bucket 3: 3000 séquences, longueurs [50-264], moyenne: 108.5\n",
      "Bucket 4: 4000 séquences, longueurs [50-264], moyenne: 108.9\n",
      "\n",
      "Bucket 1/4:\n",
      "  Train: 1000 séquences\n",
      "  Validation: 250 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 3625.876209, Val loss/event: 1380.505983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 805.707865, Val loss/event: 401.072957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 297.115288, Val loss/event: 189.934524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 155.825177, Val loss/event: 111.020641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 96.501975, Val loss/event: 73.312328\n",
      "\n",
      "Bucket 2/4:\n",
      "  Train: 2000 séquences\n",
      "  Validation: 500 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 55.640472, Val loss/event: 41.713756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 33.557636, Val loss/event: 27.520846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 23.093477, Val loss/event: 19.557973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 17.256192, Val loss/event: 15.418177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 13.972458, Val loss/event: 12.805040\n",
      "\n",
      "Bucket 3/4:\n",
      "  Train: 3000 séquences\n",
      "  Validation: 750 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 11.574459, Val loss/event: 10.404471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 9.832322, Val loss/event: 9.172510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 8.798173, Val loss/event: 8.365095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 8.138382, Val loss/event: 7.920102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 7.712545, Val loss/event: 7.515087\n",
      "\n",
      "Bucket 4/4:\n",
      "  Train: 4000 séquences\n",
      "  Validation: 1000 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 7.380974, Val loss/event: 7.129827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 7.101854, Val loss/event: 7.009376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 6.915888, Val loss/event: 6.863782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 6.811480, Val loss/event: 6.742955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 6.709616, Val loss/event: 6.666059\n",
      "Modèle sauvegardé: Comparison_ModelsV6/Random_Buckets_finalV6.pt\n",
      "\n",
      "================================================================================\n",
      "ENTRAÎNEMENT: Progressive Random\n",
      "================================================================================\n",
      "Bucket 1: 1000 séquences, longueurs [50-262], moyenne: 106.2\n",
      "Bucket 2: 2000 séquences, longueurs [50-264], moyenne: 108.9\n",
      "Bucket 3: 3000 séquences, longueurs [50-264], moyenne: 108.5\n",
      "Bucket 4: 4000 séquences, longueurs [50-264], moyenne: 108.9\n",
      "\n",
      "Bucket 1/4:\n",
      "  Train: 1000 séquences\n",
      "  Validation: 250 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 3582.795700, Val loss/event: 1368.625016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 796.793400, Val loss/event: 402.568059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 296.400235, Val loss/event: 190.094278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 155.713800, Val loss/event: 111.370708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 96.559615, Val loss/event: 73.416064\n",
      "\n",
      "Bucket 2/4:\n",
      "  Train: 2000 séquences\n",
      "  Validation: 500 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 55.299167, Val loss/event: 41.759506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 33.335260, Val loss/event: 27.268678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 22.889460, Val loss/event: 19.633966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 17.169667, Val loss/event: 15.434573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 13.902666, Val loss/event: 12.836127\n",
      "\n",
      "Bucket 3/4:\n",
      "  Train: 3000 séquences\n",
      "  Validation: 750 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 11.495234, Val loss/event: 10.445231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 9.790881, Val loss/event: 9.160132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 8.758930, Val loss/event: 8.413673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 8.149623, Val loss/event: 7.896750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 7.700866, Val loss/event: 7.573184\n",
      "\n",
      "Bucket 4/4:\n",
      "  Train: 4000 séquences\n",
      "  Validation: 1000 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 7.376791, Val loss/event: 7.211296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 7.103986, Val loss/event: 6.975849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 6.943434, Val loss/event: 6.871018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 6.834133, Val loss/event: 6.767989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 6.741092, Val loss/event: 6.651325\n",
      "Modèle sauvegardé: Comparison_ModelsV6/Progressive_Random_finalV6.pt\n",
      "\n",
      "================================================================================\n",
      "ENTRAÎNEMENT: Curriculum Learning\n",
      "================================================================================\n",
      "Bucket 1: 1000 séquences, longueurs [50-65], moyenne: 57.1\n",
      "Bucket 2: 2000 séquences, longueurs [50-89], moyenne: 66.5\n",
      "Bucket 3: 3000 séquences, longueurs [50-140], moyenne: 81.6\n",
      "Bucket 4: 4000 séquences, longueurs [50-264], moyenne: 108.9\n",
      "\n",
      "Bucket 1/4:\n",
      "  Train: 1000 séquences\n",
      "  Validation: 250 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 4220.330992, Val loss/event: 1440.049564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 964.533282, Val loss/event: 422.270584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 356.435281, Val loss/event: 199.591010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 186.257345, Val loss/event: 116.437946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 115.022769, Val loss/event: 76.406115\n",
      "\n",
      "Bucket 2/4:\n",
      "  Train: 2000 séquences\n",
      "  Validation: 500 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 66.435830, Val loss/event: 39.359695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 36.778626, Val loss/event: 24.796854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 24.315924, Val loss/event: 17.615473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 17.971484, Val loss/event: 13.909496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 14.417856, Val loss/event: 11.734041\n",
      "\n",
      "Bucket 3/4:\n",
      "  Train: 3000 séquences\n",
      "  Validation: 750 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 11.347039, Val loss/event: 9.508905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 9.549544, Val loss/event: 8.450571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 8.604074, Val loss/event: 7.827674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 8.020574, Val loss/event: 7.496386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 7.624585, Val loss/event: 7.215824\n",
      "\n",
      "Bucket 4/4:\n",
      "  Train: 4000 séquences\n",
      "  Validation: 1000 séquences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: Train loss/event: 7.094023, Val loss/event: 6.946991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2: Train loss/event: 6.867474, Val loss/event: 6.794487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3: Train loss/event: 6.760833, Val loss/event: 6.704490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: Train loss/event: 6.681130, Val loss/event: 6.649036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5: Train loss/event: 6.626893, Val loss/event: 6.570908\n",
      "Modèle sauvegardé: Comparison_ModelsV6/Curriculum_Learning_finalV6.pt\n",
      "\n",
      "Résultats sauvegardés dans 'training_strategies_comparison.npy'\n",
      "Comparaison terminée!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch as pt\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from LSTM_construction import CTLSTM\n",
    "import os\n",
    "\n",
    "def prepare_bucket_tensors(bucket_data, N_types=3):\n",
    "    \"\"\"\n",
    "    Convertit les données d'un bucket en tenseurs PyTorch\n",
    "    \"\"\"\n",
    "    EventsData = bucket_data['EventsData']\n",
    "    SeqLengthData = bucket_data['SeqLengthData']\n",
    "    timesData = bucket_data['timesData']\n",
    "    timeMaxData = bucket_data['timeMaxData']\n",
    "    \n",
    "    N_train = EventsData.shape[0]\n",
    "    N_seq_Max = EventsData.shape[1]\n",
    "    \n",
    "    # Créer les événements one-hot\n",
    "    Events_one_hot = np.zeros((N_train, N_seq_Max, N_types))\n",
    "    for seq in range(N_train):\n",
    "        for step in range(SeqLengthData[seq]):\n",
    "            ev = EventsData[seq, step]\n",
    "            if ev >= 0:  # Événement valide\n",
    "                Events_one_hot[seq, step, ev] = 1.0\n",
    "    \n",
    "    # Convertir en tenseurs PyTorch\n",
    "    timeScale = 1.0\n",
    "    EvTens = pt.tensor(Events_one_hot).double()\n",
    "    EvIndTens = pt.tensor(EventsData).long()\n",
    "    timeTensor = pt.tensor(timesData/timeScale).double()\n",
    "    tMaxTensor = pt.tensor(timeMaxData/timeScale).double()\n",
    "    mask = EvIndTens.ge(-1+0.001)\n",
    "    \n",
    "    return EvTens, EvIndTens, timeTensor, tMaxTensor, mask, SeqLengthData\n",
    "\n",
    "def evaluate_on_validation(net, EvTens_dev, EvIndTens_dev, timeTensor_dev, tMaxTensor_dev, mask_dev, batch_size=10):\n",
    "    \"\"\"\n",
    "    Évalue le modèle sur les données de validation\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    total_dev_loss = 0.0\n",
    "    N_dev = len(EvTens_dev)\n",
    "    N_seq_Max = EvTens_dev.shape[1]\n",
    "    \n",
    "    with pt.no_grad():\n",
    "        for batchInd in range(0, N_dev, batch_size):\n",
    "            batch_end = min(batchInd + batch_size, N_dev)\n",
    "            BatchEventsHot_dev = EvTens_dev[batchInd:batch_end]\n",
    "            BatchEventsInd_dev = EvIndTens_dev[batchInd:batch_end]\n",
    "            BatchTimes_dev = timeTensor_dev[batchInd:batch_end]\n",
    "            BatchTMax_dev = tMaxTensor_dev[batchInd:batch_end]\n",
    "            BatchMask_dev = mask_dev[batchInd:batch_end]\n",
    "            \n",
    "            # Forward pass\n",
    "            lambOuts_dev, CLows_dev, Cbars_dev, deltas_dev, OutGates_dev = net.forward(\n",
    "                BatchEventsHot_dev, BatchMask_dev, BatchTimes_dev\n",
    "            )\n",
    "            \n",
    "            # Calculate MC loss\n",
    "            LMC_dev, _, _, _ = net.MC_Loss(\n",
    "                BatchTimes_dev, BatchTMax_dev, CLows_dev, Cbars_dev, \n",
    "                deltas_dev, OutGates_dev, Nsamples=N_seq_Max\n",
    "            )\n",
    "            \n",
    "            # Calculate likelihood loss\n",
    "            LogLikeLoss_dev, _ = net.logLoss(BatchEventsInd_dev, BatchMask_dev, lambOuts_dev)\n",
    "            \n",
    "            loss_dev = LogLikeLoss_dev + LMC_dev\n",
    "            total_dev_loss += loss_dev.item()*(BatchEventsInd_dev.shape[0])\n",
    "    \n",
    "    return total_dev_loss\n",
    "\n",
    "def train_bucket(net, optimizer, EvTens, EvIndTens, timeTensor, tMaxTensor, mask, \n",
    "                SeqLengthData, EvTens_val, EvIndTens_val, timeTensor_val, tMaxTensor_val, mask_val, SeqLengthData_val,\n",
    "                bucket_idx, num_epochs=5, batch_size=10):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle sur un bucket pour un nombre fixe d'époques\n",
    "    MODIFICATION: Calcule la validation loss après chaque époque\n",
    "    \"\"\"\n",
    "    N_train = len(EvTens)\n",
    "    N_seq_Max = EvTens.shape[1]\n",
    "    bucket_train_losses = []\n",
    "    bucket_val_losses = []\n",
    "    sequence_losses_per_epoch = {}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_total_losses = []\n",
    "        epoch_losses_dict = {}\n",
    "        \n",
    "        # Permutation pour cette époque\n",
    "        perm = pt.randperm(N_train)\n",
    "        \n",
    "        progress_bar = tqdm(range(0, N_train, batch_size), \n",
    "                           desc=f\"Bucket {bucket_idx+1}, Epoch {epoch+1}/{num_epochs}\", \n",
    "                           leave=False)\n",
    "        \n",
    "        for batchInd in progress_bar:\n",
    "            batch_end = min(batchInd + batch_size, N_train)\n",
    "            batch_indices = perm[batchInd:batch_end]\n",
    "            \n",
    "            BatchEventsHot = EvTens[batch_indices]\n",
    "            BatchEventsInd = EvIndTens[batch_indices]\n",
    "            BatchTimes = timeTensor[batch_indices]\n",
    "            BatchTMax = tMaxTensor[batch_indices]\n",
    "            BatchMask = mask[batch_indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            lambOuts, CLows, Cbars, deltas, OutGates = net.forward(\n",
    "                BatchEventsHot, BatchMask, BatchTimes\n",
    "            )\n",
    "            \n",
    "            # Calculate losses\n",
    "            LMC, _, _ ,individual_mc_losses= net.MC_Loss(\n",
    "                BatchTimes, BatchTMax, CLows, Cbars, deltas, OutGates,\n",
    "                Nsamples=N_seq_Max\n",
    "            )\n",
    "            LogLikeLoss, individual_log_losses = net.logLoss(BatchEventsInd, BatchMask, lambOuts)\n",
    "\n",
    "            \n",
    "            loss = LogLikeLoss + LMC\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            individual_total_losses = [mc + log for mc, log in zip(individual_mc_losses, individual_log_losses)]\n",
    "            # MODIFICATION: Mapper chaque loss à son indice original\n",
    "            for i, total_loss in enumerate(individual_total_losses):\n",
    "                original_idx = batch_indices[i].item()\n",
    "                epoch_losses_dict[original_idx] = total_loss       \n",
    "\n",
    "            \n",
    "            epoch_loss += loss.item()*(BatchEventsInd.shape[0])\n",
    "            progress_bar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculer la loss per event pour cette époque\n",
    "        train_loss_per_event = epoch_loss / np.sum(SeqLengthData)\n",
    "        bucket_train_losses.append(train_loss_per_event)\n",
    "        epoch_total_losses = [epoch_losses_dict[i] for i in range(N_train)]\n",
    "        # NOUVELLE PARTIE: Évaluer sur validation après chaque époque\n",
    "        val_loss = evaluate_on_validation(\n",
    "            net, EvTens_val, EvIndTens_val, timeTensor_val, tMaxTensor_val, mask_val\n",
    "        )\n",
    "        val_loss_per_event = val_loss / np.sum(SeqLengthData_val)\n",
    "        bucket_val_losses.append(val_loss_per_event)\n",
    "        sequence_losses_per_epoch[epoch] = epoch_total_losses\n",
    "        \n",
    "        print(f\"    Epoch {epoch+1}: Train loss/event: {train_loss_per_event:.6f}, Val loss/event: {val_loss_per_event:.6f}\")\n",
    "    \n",
    "    return bucket_train_losses, bucket_val_losses, sequence_losses_per_epoch\n",
    "\n",
    "# ============================================================================\n",
    "# STRATÉGIE 1: CURRICULUM LEARNING (ordre selon le loss per event de l'epoch 2)\n",
    "# ============================================================================\n",
    "sequence_data = np.load('sequence_losses_per_epochV5.npy', allow_pickle=True).item()\n",
    "with h5py.File(\"RetweetTrainData.h5\", \"r\") as fl:\n",
    "    SeqLengthData = np.array(fl[\"SeqLengthData\"])\n",
    "sequence_lengths = SeqLengthData[:4000]\n",
    "epoch_2_losses = sequence_data[1]/sequence_lengths\n",
    "\n",
    "def create_curriculum_bucketsV2(EventsData, SeqLengthData, timesData, timeMaxData, epoch_2_losses, num_buckets=4):\n",
    "    \"\"\"\n",
    "    Curriculum Learning: Divise selon les losses de l'epoch 2 (ordonné) avec baby steps\n",
    "    \"\"\"\n",
    "    # Trier les indices par losses croissantes\n",
    "    sorted_indices = np.argsort(epoch_2_losses)\n",
    "    \n",
    "    # Diviser en buckets de base\n",
    "    N_sequences = len(epoch_2_losses)\n",
    "    bucket_size = N_sequences // num_buckets\n",
    "    \n",
    "    base_buckets = []\n",
    "    for i in range(num_buckets):\n",
    "        start_idx = i * bucket_size\n",
    "        if i == num_buckets - 1:\n",
    "            end_idx = N_sequences\n",
    "        else:\n",
    "            end_idx = (i + 1) * bucket_size\n",
    "        base_buckets.append(sorted_indices[start_idx:end_idx])\n",
    "    \n",
    "    # Créer les buckets cumulatifs (baby steps)\n",
    "    curriculum_buckets = []\n",
    "    cumulative_indices = np.array([], dtype=int)\n",
    "    \n",
    "    for i, bucket in enumerate(base_buckets):\n",
    "        cumulative_indices = np.concatenate([cumulative_indices, bucket])\n",
    "        bucket_data = {\n",
    "            'EventsData': EventsData[cumulative_indices],\n",
    "            'SeqLengthData': SeqLengthData[cumulative_indices],\n",
    "            'timesData': timesData[cumulative_indices],\n",
    "            'timeMaxData': timeMaxData[cumulative_indices],\n",
    "            'indices': cumulative_indices.copy()\n",
    "        }\n",
    "        curriculum_buckets.append(bucket_data)\n",
    "    \n",
    "    return curriculum_buckets\n",
    "\n",
    "# ============================================================================\n",
    "# STRATÉGIE 2: RANDOM BUCKETS (mêmes tailles, ordre aléatoire)\n",
    "# ============================================================================\n",
    "\n",
    "def create_random_buckets(EventsData, SeqLengthData, timesData, timeMaxData, num_buckets=4, seed=42):\n",
    "    \"\"\"\n",
    "    Sélection laléatoire cumulative  \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N_sequences = len(SeqLengthData)\n",
    "    \n",
    "    # Mélanger aléatoirement les indices\n",
    "    shuffled_indices = np.random.permutation(N_sequences)\n",
    "    \n",
    "    # Diviser en buckets de même taille\n",
    "    bucket_size = N_sequences // num_buckets\n",
    "    \n",
    "    base_buckets = []\n",
    "    for i in range(num_buckets):\n",
    "        start_idx = i * bucket_size\n",
    "        if i == num_buckets - 1:\n",
    "            end_idx = N_sequences\n",
    "        else:\n",
    "            end_idx = (i + 1) * bucket_size\n",
    "        base_buckets.append(shuffled_indices[start_idx:end_idx])\n",
    "    \n",
    "    # Créer les buckets cumulatifs (baby steps)\n",
    "    random_buckets = []\n",
    "    cumulative_indices = np.array([], dtype=int)\n",
    "    \n",
    "    for i, bucket in enumerate(base_buckets):\n",
    "        cumulative_indices = np.concatenate([cumulative_indices, bucket])\n",
    "        bucket_data = {\n",
    "            'EventsData': EventsData[cumulative_indices],\n",
    "            'SeqLengthData': SeqLengthData[cumulative_indices],\n",
    "            'timesData': timesData[cumulative_indices],\n",
    "            'timeMaxData': timeMaxData[cumulative_indices],\n",
    "            'indices': cumulative_indices.copy()\n",
    "        }\n",
    "        random_buckets.append(bucket_data)\n",
    "    \n",
    "    return random_buckets\n",
    "\n",
    "# ============================================================================\n",
    "# STRATÉGIE 3: PROGRESSIVE RANDOM (25%, 50%, 75%, 100%) totally random\n",
    "# ============================================================================\n",
    "\n",
    "def create_progressive_buckets(EventsData, SeqLengthData, timesData, timeMaxData, num_buckets=4, seed=42):\n",
    "    \"\"\"\n",
    "    Progressive: Sélection aléatoire progressive (25%, 50%, 75%, 100%)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N_sequences = len(SeqLengthData)\n",
    "    \n",
    "    progressive_buckets = []\n",
    "    \n",
    "    for i in range(num_buckets):\n",
    "        # Calculer le pourcentage: 25%, 50%, 75%, 100%\n",
    "        percentage = (i + 1) / num_buckets\n",
    "        num_sequences_in_bucket = int(N_sequences * percentage)\n",
    "        \n",
    "        # Générer de nouveaux indices aléatoires pour chaque bucket\n",
    "        # Utiliser un seed différent pour chaque bucket\n",
    "        np.random.seed(seed + i * 100)  # Seed différent pour chaque bucket\n",
    "        bucket_indices = np.random.choice(N_sequences, num_sequences_in_bucket, replace=False)\n",
    "        \n",
    "        bucket_data = {\n",
    "            'EventsData': EventsData[bucket_indices],\n",
    "            'SeqLengthData': SeqLengthData[bucket_indices],\n",
    "            'timesData': timesData[bucket_indices],\n",
    "            'timeMaxData': timeMaxData[bucket_indices],\n",
    "            'indices': bucket_indices.copy()\n",
    "        }\n",
    "        progressive_buckets.append(bucket_data)\n",
    "    \n",
    "    return progressive_buckets\n",
    "\n",
    "# ============================================================================\n",
    "# STRATÉGIE 1: CURRICULUM LEARNING (ordre selon la longueur, ordonné)\n",
    "# ============================================================================\n",
    "def create_curriculum_buckets(EventsData, SeqLengthData, timesData, timeMaxData, num_buckets=4):\n",
    "    \"\"\"\n",
    "    Curriculum Learning: Divise selon la longueur (ordonné) avec baby steps\n",
    "    \"\"\"\n",
    "    # Trier les indices par longueur croissante\n",
    "    sorted_indices = np.argsort(SeqLengthData)\n",
    "    \n",
    "    # Diviser en buckets de base\n",
    "    N_sequences = len(SeqLengthData)\n",
    "    bucket_size = N_sequences // num_buckets\n",
    "    \n",
    "    base_buckets = []\n",
    "    for i in range(num_buckets):\n",
    "        start_idx = i * bucket_size\n",
    "        if i == num_buckets - 1:\n",
    "            end_idx = N_sequences\n",
    "        else:\n",
    "            end_idx = (i + 1) * bucket_size\n",
    "        base_buckets.append(sorted_indices[start_idx:end_idx])\n",
    "    \n",
    "    # Créer les buckets cumulatifs (baby steps)\n",
    "    curriculum_buckets = []\n",
    "    cumulative_indices = np.array([], dtype=int)\n",
    "    \n",
    "    for i, bucket in enumerate(base_buckets):\n",
    "        cumulative_indices = np.concatenate([cumulative_indices, bucket])\n",
    "        bucket_data = {\n",
    "            'EventsData': EventsData[cumulative_indices],\n",
    "            'SeqLengthData': SeqLengthData[cumulative_indices],\n",
    "            'timesData': timesData[cumulative_indices],\n",
    "            'timeMaxData': timeMaxData[cumulative_indices],\n",
    "            'indices': cumulative_indices.copy()\n",
    "        }\n",
    "        curriculum_buckets.append(bucket_data)\n",
    "    \n",
    "    return curriculum_buckets\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTION PRINCIPALE D'ENTRAÎNEMENT ET COMPARAISON\n",
    "# ============================================================================\n",
    "\n",
    "def compare_training_strategies(file_path=\"RetweetTrainData.h5\", dev_file_path=\"RetweetDevData.h5\", \n",
    "                              N_train=4000, num_buckets=4, hD=16, lr=0.001, epochs_per_bucket=5):\n",
    "    \"\"\"\n",
    "    Compare les 4 stratégies d'entraînement\n",
    "    MODIFICATION: Validation loss calculée après chaque époque\n",
    "    \"\"\"\n",
    "    print(\"Chargement des données...\")\n",
    "    \n",
    "    # Charger les données d'entraînement\n",
    "    with h5py.File(file_path, \"r\") as fl:\n",
    "        EventsData = np.array(fl[\"EventsData\"])[:N_train]\n",
    "        timesData = np.array(fl[\"TimesData\"])[:N_train]\n",
    "        timeMaxData = np.array(fl[\"TimeMaxData\"])[:N_train]\n",
    "        SeqLengthData = np.array(fl[\"SeqLengthData\"])[:N_train]\n",
    "    \n",
    "    # Charger les données de validation\n",
    "    with h5py.File(dev_file_path, \"r\") as fl:\n",
    "        EventsData_dev = np.array(fl[\"EventsData\"])\n",
    "        timesData_dev = np.array(fl[\"TimesData\"])\n",
    "        timeMaxData_dev = np.array(fl[\"TimeMaxData\"])\n",
    "        SeqLengthData_dev = np.array(fl[\"SeqLengthData\"])\n",
    "    \n",
    "    print(f\"Données chargées: {N_train} train, {len(EventsData_dev)} validation\\n\")\n",
    "    \n",
    "    # Créer les trois types de buckets\n",
    "    print(\"Création des buckets pour les 4 stratégies...\")\n",
    "    curriculum_bucketsV2 = create_curriculum_bucketsV2(EventsData, SeqLengthData, timesData, timeMaxData, epoch_2_losses, num_buckets)\n",
    "    random_buckets = create_random_buckets(EventsData, SeqLengthData, timesData, timeMaxData, num_buckets, seed=42)\n",
    "    progressive_buckets = create_progressive_buckets(EventsData, SeqLengthData, timesData, timeMaxData, num_buckets, seed=42)\n",
    "    curriculum_buckets= create_curriculum_buckets(EventsData, SeqLengthData, timesData, timeMaxData, num_buckets)\n",
    "    \n",
    "    strategies = [\n",
    "        (\"Curriculum LearningV2\", curriculum_bucketsV2),\n",
    "        (\"Random Buckets\", random_buckets), \n",
    "        (\"Progressive Random\", progressive_buckets),\n",
    "        (\"Curriculum Learning\", curriculum_buckets)\n",
    "    ]\n",
    "    \n",
    "    # Résultats pour chaque stratégie\n",
    "    all_results = {}\n",
    "    \n",
    "    # Créer le dossier pour sauvegarder les modèles\n",
    "    os.makedirs('Comparison_ModelsV6', exist_ok=True)\n",
    "    \n",
    "    for strategy_name, buckets in strategies:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ENTRAÎNEMENT: {strategy_name}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        # Analyser les buckets\n",
    "        for i, bucket in enumerate(buckets):\n",
    "            lengths = bucket['SeqLengthData']\n",
    "            print(f\"Bucket {i+1}: {len(bucket['indices'])} séquences, \"\n",
    "                  f\"longueurs [{lengths.min()}-{lengths.max()}], moyenne: {lengths.mean():.1f}\")\n",
    "        \n",
    "        # Initialiser le modèle\n",
    "        net = CTLSTM(K=3, hD=hD).double()\n",
    "        optimizer = pt.optim.Adam(net.parameters(), lr=lr)\n",
    "        \n",
    "        # Variables pour tracking\n",
    "        train_losses_per_event = []\n",
    "        val_losses_per_event = []\n",
    "        \n",
    "\n",
    "        # Entraîner bucket par bucket\n",
    "        for bucket_idx, bucket_data in enumerate(buckets):\n",
    "            print(f\"\\nBucket {bucket_idx+1}/{num_buckets}:\")\n",
    "            \n",
    "            # Préparer les tenseurs pour ce bucket\n",
    "            EvTens, EvIndTens, timeTensor, tMaxTensor, mask, SeqLengthData_bucket = prepare_bucket_tensors(bucket_data)\n",
    "            \n",
    "            # Nombre de séquences de validation = nombre de séquences train / 4\n",
    "            n_val_sequences = len(bucket_data['indices']) // 4\n",
    "            n_val_sequences = min(n_val_sequences, len(EventsData_dev))  # Ne pas dépasser la taille des données de validation\n",
    "            \n",
    "            # Préparer les données de validation (sous-échantillon)\n",
    "            val_bucket_data = {\n",
    "                'EventsData': EventsData_dev[:n_val_sequences],\n",
    "                'SeqLengthData': SeqLengthData_dev[:n_val_sequences],\n",
    "                'timesData': timesData_dev[:n_val_sequences], \n",
    "                'timeMaxData': timeMaxData_dev[:n_val_sequences],\n",
    "            }\n",
    "            EvTens_val, EvIndTens_val, timeTensor_val, tMaxTensor_val, mask_val, SeqLengthData_val = prepare_bucket_tensors(val_bucket_data)\n",
    "            \n",
    "            print(f\"  Train: {len(bucket_data['indices'])} séquences\")\n",
    "            print(f\"  Validation: {n_val_sequences} séquences\")\n",
    "            \n",
    "            # Entraîner sur ce bucket (MODIFICATION: retourne aussi val losses)\n",
    "            bucket_train_losses, bucket_val_losses, _= train_bucket(\n",
    "                net, optimizer, EvTens, EvIndTens, timeTensor, tMaxTensor, mask, SeqLengthData_bucket,\n",
    "                EvTens_val, EvIndTens_val, timeTensor_val, tMaxTensor_val, mask_val, SeqLengthData_val,\n",
    "                bucket_idx, num_epochs=epochs_per_bucket, batch_size=10\n",
    "            )\n",
    "            \n",
    "            # Ajouter les losses de ce bucket aux listes globales\n",
    "            train_losses_per_event.extend(bucket_train_losses)\n",
    "            val_losses_per_event.extend(bucket_val_losses)\n",
    "            \n",
    "        \n",
    "        # Sauvegarder le modèle final\n",
    "        model_path = f'Comparison_ModelsV6/{strategy_name.replace(\" \", \"_\")}_finalV6.pt'\n",
    "        pt.save(net, model_path)\n",
    "        print(f\"Modèle sauvegardé: {model_path}\")\n",
    "        \n",
    "        # Stocker les résultats\n",
    "        all_results[strategy_name] = {\n",
    "            'train_losses_per_event': train_losses_per_event,\n",
    "            'val_losses_per_event': val_losses_per_event\n",
    "        }\n",
    "    \n",
    "    # Créer les graphiques de comparaison\n",
    "    # create_comparison_plots(all_results, num_buckets, epochs_per_bucket)\n",
    "    \n",
    "    # Sauvegarder tous les résultats\n",
    "    np.save('training_strategies_comparisonV6.npy', all_results)\n",
    "    print(\"\\nRésultats sauvegardés dans 'training_strategies_comparison.npy'\")\n",
    "\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTION PRINCIPALE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Comparaison de 4 stratégies d'entraînement\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Lancer la comparaison\n",
    "    results = compare_training_strategies(\n",
    "        file_path=\"RetweetTrainData.h5\",\n",
    "        dev_file_path=\"RetweetDevData.h5\",\n",
    "        N_train=4000,\n",
    "        num_buckets=4,\n",
    "        hD=16,\n",
    "        lr=0.001,\n",
    "        epochs_per_bucket=5\n",
    "    )\n",
    "    \n",
    "    print(\"Comparaison terminée!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
